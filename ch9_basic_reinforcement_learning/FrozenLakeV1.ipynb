{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FrozenLakeV1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNJ41jhQkHkq7BIMp6AzjdE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wesley34/comp3414_course_material/blob/master/ch_9_basic_reinforcement_learning/FrozenLakeV1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3wkZWK45rFE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "outputId": "0bd2b54d-7093-490c-dcd4-13009b6139d0"
      },
      "source": [
        "!pip install tensorboardX"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n",
            "\r\u001b[K     |█                               | 10kB 20.5MB/s eta 0:00:01\r\u001b[K     |██▏                             | 20kB 6.3MB/s eta 0:00:01\r\u001b[K     |███▏                            | 30kB 7.6MB/s eta 0:00:01\r\u001b[K     |████▎                           | 40kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 51kB 6.7MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 61kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 71kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 81kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 92kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 102kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 112kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 122kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 133kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 143kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████                | 153kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 163kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 174kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 184kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 194kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 204kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 215kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 225kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 235kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 245kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 256kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 266kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 276kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 286kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 296kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 307kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 317kB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.12.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (49.6.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMOZNn3Li2z-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import gym.envs.toy_text.frozen_lake\n",
        "import gym.wrappers\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import namedtuple\n",
        "import numpy as np\n",
        "from tensorboardX import SummaryWriter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llF2taL8BnOP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HIDDEN_SIZE = 128\n",
        "BATCH_SIZE = 100\n",
        "PERCENTILE = 30\n",
        "GAMMA = 0.9"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3BdHfD859H2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### wrapper for discrete sample space\n",
        "class DiscreteOneHotWrapper(gym.ObservationWrapper):\n",
        "  def __init__(self,env):\n",
        "    super(DiscreteOneHotWrapper,self).__init__(env)\n",
        "    assert isinstance(env.observation_space,gym.spaces.Discrete)\n",
        "    self.observation_space = gym.spaces.Box(0.0,1.0,(env.observation_space.n,),dtype=np.float32)\n",
        "  \n",
        "  #overriden\n",
        "  def observation(self,observation):\n",
        "    result = np.copy(self.observation_space.low)\n",
        "    result[observation] = 1.0\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eD_UzrGi68pC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### simple neural network\n",
        "class Net(nn.Module):\n",
        "  def __init__(self,obs_size,hidden_size,action_size):\n",
        "    super(Net,self).__init__()\n",
        "    self.network = nn.Sequential(\n",
        "        nn.Linear(obs_size,hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size,action_size)\n",
        "    )\n",
        "  ## overriden\n",
        "  def forward(self,x):\n",
        "    return self.network(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LxCHyNH8NB_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# classes for tupple\n",
        "Episode = namedtuple(\"Episode\",[\"reward\",\"step\"])\n",
        "Episode_step = namedtuple(\"EpisodeStep\",[\"action\",\"observation\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNhCDzuD7YKI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### training on network + random_walk for the data\n",
        "def iteration_batch(net,env,batch_size):\n",
        "  episode_reward = 0.0\n",
        "  batch = []\n",
        "  episode_step = []\n",
        "  obs = env.reset()\n",
        "  softmax = nn.Softmax(dim=1)\n",
        "  # 3 elements for making a dataset for RL\n",
        "  while True:\n",
        "    obs_vector = torch.FloatTensor([obs])\n",
        "    action_score = net(obs_vector)\n",
        "    action_probability_vector = softmax(action_score)\n",
        "    action_probability = action_probability_vector.data.numpy()[0]\n",
        "    action = np.random.choice(len(action_probability),p=action_probability)\n",
        "    next_obs, reward, is_done, _ = env.step(action)\n",
        "    episode_reward += reward\n",
        "    episode_step.append(Episode_step(action=action,observation=obs))\n",
        "    if is_done:\n",
        "      batch.append(Episode(reward=reward,step=episode_step))\n",
        "      episode_reward = 0.0\n",
        "      episode_step = []\n",
        "      next_obs = env.reset() #important\n",
        "      if len(batch) == batch_size:\n",
        "        yield batch\n",
        "        batch = []\n",
        "      \n",
        "    obs = next_obs\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tG4KpvYm9Ppy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## filter for elite batch\n",
        "def filter_batch(batch,percentile):\n",
        "  \n",
        "  reward = list(map(lambda sample: sample.reward * (GAMMA**len(sample.step)),batch))\n",
        "\n",
        "  reward_bound = np.percentile(reward,percentile)\n",
        "\n",
        "  train_obs = []\n",
        "  train_action = []\n",
        "  train_elite = []\n",
        "\n",
        "  for example,discount_reward in zip(batch,reward):\n",
        "\n",
        "    if discount_reward > reward_bound:\n",
        "      train_obs.extend(list(map(lambda step : step.observation,example.step)))\n",
        "      train_action.extend(list(map(lambda step : step.action,example.step)))\n",
        "      train_elite.append(example)\n",
        "\n",
        "  return train_elite,train_obs,train_action,reward_bound\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2bMycYU9VSm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "316632fb-9b74-49e4-fb09-1c532c2bb46e"
      },
      "source": [
        "## training pharse\n",
        "full_batch = []\n",
        "env = gym.envs.toy_text.frozen_lake.FrozenLakeEnv(is_slippery=False)\n",
        "env = gym.wrappers.TimeLimit(env,max_episode_steps=100)\n",
        "env = DiscreteOneHotWrapper(env)\n",
        "net = Net(env.observation_space.shape[0],HIDDEN_SIZE,env.action_space.n)\n",
        "optimizer = optim.Adam(params=net.parameters(),lr=0.01)\n",
        "objective = nn.CrossEntropyLoss()\n",
        "softmax = nn.Softmax(dim=1)\n",
        "writer = SummaryWriter(comment=\"-frozenlake-nonslippery\")\n",
        "\n",
        "for iter_no , batch in enumerate(iteration_batch(net,env,BATCH_SIZE)):\n",
        "  reward_mean = float(np.mean(list(map(lambda step : step.reward,batch))))\n",
        "  full_batch,train_obs,train_action,reward_bound = filter_batch(full_batch+batch,PERCENTILE)\n",
        "  if not full_batch:\n",
        "    continue\n",
        "  obs_vector = torch.FloatTensor(train_obs)\n",
        "  action_vector = torch.LongTensor(train_action)\n",
        "  full_batch = full_batch[-500:]\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  action_probability = net(obs_vector)\n",
        "  \n",
        "  loss_vector = objective(action_probability,action_vector)\n",
        "  loss_vector.backward()\n",
        "  optimizer.step()\n",
        "  print(\"%d: loss=%.3f, reward_mean=%.1f, reward_bound=%.1f\" % (\n",
        "          iter_no, loss_vector.item(), reward_mean, reward_bound))\n",
        "  \n",
        "  writer.add_scalar(\"loss\", loss_vector.item(), iter_no)\n",
        "  writer.add_scalar(\"reward_bound\", reward_bound, iter_no)\n",
        "  writer.add_scalar(\"reward_mean\", reward_mean, iter_no)\n",
        "  if reward_mean > 0.89:\n",
        "      print(\"Solved!\")\n",
        "      break\n",
        "  writer.close()\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: loss=1.383, reward_mean=0.0, reward_bound=0.0\n",
            "1: loss=1.352, reward_mean=0.0, reward_bound=0.0\n",
            "2: loss=1.331, reward_mean=0.0, reward_bound=0.0\n",
            "3: loss=1.297, reward_mean=0.0, reward_bound=0.0\n",
            "4: loss=1.273, reward_mean=0.1, reward_bound=0.0\n",
            "5: loss=1.228, reward_mean=0.1, reward_bound=0.0\n",
            "6: loss=1.208, reward_mean=0.1, reward_bound=0.0\n",
            "7: loss=1.159, reward_mean=0.2, reward_bound=0.0\n",
            "8: loss=1.114, reward_mean=0.2, reward_bound=0.0\n",
            "9: loss=1.080, reward_mean=0.2, reward_bound=0.0\n",
            "10: loss=1.047, reward_mean=0.2, reward_bound=0.0\n",
            "11: loss=0.970, reward_mean=0.4, reward_bound=0.1\n",
            "12: loss=0.878, reward_mean=0.5, reward_bound=0.2\n",
            "13: loss=0.737, reward_mean=0.6, reward_bound=0.3\n",
            "14: loss=0.652, reward_mean=0.6, reward_bound=0.3\n",
            "15: loss=0.552, reward_mean=0.7, reward_bound=0.4\n",
            "16: loss=0.400, reward_mean=0.8, reward_bound=0.4\n",
            "17: loss=0.223, reward_mean=0.8, reward_bound=0.5\n",
            "19: loss=0.246, reward_mean=0.9, reward_bound=0.5\n",
            "Solved!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akLIaGfTCA5d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
      
    }
  ]
}
