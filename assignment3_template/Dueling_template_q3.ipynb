{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dueling_template_q3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOkDFpg90ORI6dL3Z+uzj77",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wesley34/comp3414_course_material/blob/master/assignment3_template/Dueling_template_q3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zClS9Wj-gy6h",
        "outputId": "98e72c23-7707-4960-a321-c4895fa1c274",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        }
      },
      "source": [
        "# N-step , Dueling DQN\n",
        "!pip install tensorboardX"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n",
            "\r\u001b[K     |█                               | 10kB 14.4MB/s eta 0:00:01\r\u001b[K     |██▏                             | 20kB 2.8MB/s eta 0:00:01\r\u001b[K     |███▏                            | 30kB 3.6MB/s eta 0:00:01\r\u001b[K     |████▎                           | 40kB 3.9MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 51kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 61kB 3.8MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 71kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 81kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 92kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 102kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 112kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 122kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 133kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 143kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████                | 153kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 163kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 174kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 184kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 194kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 204kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 215kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 225kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 235kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 245kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 256kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 266kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 276kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 286kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 296kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 307kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 317kB 4.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.18.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.12.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (50.3.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6WjCvUXlwpv",
        "outputId": "164d3c55-8076-45c1-ddc1-d55b191c8a5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        }
      },
      "source": [
        "!pip install ptan"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ptan\n",
            "  Downloading https://files.pythonhosted.org/packages/91/cb/57f6d86625f2b24c008b0524ca29559683aa75d00afa38b6b44d7fcad25b/ptan-0.6.tar.gz\n",
            "Collecting torch==1.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/05/50a05de5337f7a924bb8bd70c6936230642233e424d6a9747ef1cfbde353/torch-1.3.0-cp36-cp36m-manylinux1_x86_64.whl (773.1MB)\n",
            "\u001b[K     |████████████████████████████████| 773.1MB 23kB/s \n",
            "\u001b[?25hRequirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from ptan) (0.17.3)\n",
            "Requirement already satisfied: atari-py in /usr/local/lib/python3.6/dist-packages (from ptan) (0.2.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from ptan) (1.18.5)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from ptan) (4.1.2.30)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->ptan) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->ptan) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->ptan) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from atari-py->ptan) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->ptan) (0.16.0)\n",
            "Building wheels for collected packages: ptan\n",
            "  Building wheel for ptan (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ptan: filename=ptan-0.6-cp36-none-any.whl size=23502 sha256=cbbd6bac3a685791eea34b36d3dee94fed898bc6fa31ebc9c606bab4111d9518\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/4b/2f/9a45fd39b0a614a2716bc6128a7f1adb4647f323a2d90783f2\n",
            "Successfully built ptan\n",
            "\u001b[31mERROR: torchvision 0.7.0+cu101 has requirement torch==1.6.0, but you'll have torch 1.3.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch, ptan\n",
            "  Found existing installation: torch 1.6.0+cu101\n",
            "    Uninstalling torch-1.6.0+cu101:\n",
            "      Successfully uninstalled torch-1.6.0+cu101\n",
            "Successfully installed ptan-0.6 torch-1.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdRNnKM8hdpe"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import ptan\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "import sys\n",
        "import math\n",
        "import collections\n",
        "import time"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSzko1JthrXS"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1hrDaMtDY93"
      },
      "source": [
        ""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8R1bVDoFDY4x"
      },
      "source": [
        "## N step DQN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RN8FYkrFDZuC",
        "outputId": "8a2222d5-2c71-433a-e239-b6a2c5a1fb23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# warning : when you use n>1 , your model cannot converge to the optimal \n",
        "# but faster to converge to some point near the optimal\n",
        "n = 2\n",
        "print(\"You used {}-step DQN framework\".format(n))"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "You used 2-step DQN framework\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBGrbASTiIQY"
      },
      "source": [
        "## Dueling DQN\n",
        "We create Q-value by Q(s,a) = V(s) + A(s,a) - mean(A(s,k))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvQddtVBiJVN"
      },
      "source": [
        "class DuelingDQNModel(nn.Module):\n",
        "  def __init__(self,input_shape,n_actions):\n",
        "    super(DuelingDQNModel,self).__init__()\n",
        "\n",
        "\n",
        "    self.conv_net = nn.Sequential(\n",
        "        nn.Conv2d(input_shape[0],40,kernel_size=8,stride=3),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(40,64,kernel_size=4,stride=2),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(64,64,kernel_size=2,stride=1),\n",
        "        nn.ReLU(),\n",
        "        # nn.BatchNorm2d(),\n",
        "     \n",
        "    )\n",
        "\n",
        "    self.flattened_size = self._get_conv_output_shape(input_shape)\n",
        "\n",
        "    self.fully_connected_advantage_net = nn.Sequential(\n",
        "        nn.Linear(self.flattened_size,512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512,n_actions)\n",
        "    )\n",
        "\n",
        "    self.fully_connected_value_net = nn.Sequential(\n",
        "        nn.Linear(self.flattened_size,512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512,1),\n",
        "    )\n",
        "\n",
        " \n",
        "\n",
        "  def _get_conv_output_shape(self, shape):\n",
        "    dummy = torch.zeros(1, *shape)\n",
        "    output = self.conv_net(dummy)\n",
        "    return int(np.prod(output.size()))\n",
        "\n",
        "  #overriden\n",
        "  def forward(self,x):\n",
        "    normalized_x = x.float()/256\n",
        "    conv_out = self.conv_net(normalized_x).view(normalized_x.size()[0],-1) # flatten\n",
        "    value = self.fully_connected_value_net(conv_out)\n",
        "    advantage = self.fully_connected_advantage_net(conv_out)\n",
        "    return value+(advantage - advantage.mean(dim=1,keepdim=True)) # the equation"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTvcoMlgkRXd"
      },
      "source": [
        ""
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SPOsT1SlSyv"
      },
      "source": [
        "device setup, change run time in colab from None to GPU "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nnX2PAJlSRU"
      },
      "source": [
        "device = torch.device(\"cuda\") #cuda mean using gpu"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzX4XuZFlpuT"
      },
      "source": [
        "create gym"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phFEwMs9la--",
        "outputId": "f7dd5b35-1678-4240-86dc-330ba99e0606",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "GYM_NAME = \"PongNoFrameskip-v4\" \n",
        "env = gym.make(GYM_NAME)\n",
        "print(\"Action avaliable: \")\n",
        "env.unwrapped.get_action_meanings()"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Action avaliable: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALD5aiEAlpI_"
      },
      "source": [
        "# we will use others' wrapper \n",
        "# similar as last time\n",
        "env = ptan.common.wrappers.wrap_dqn(env)"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixXB4aZ4macx"
      },
      "source": [
        ""
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TClbJzvAnPdE"
      },
      "source": [
        "create network and target network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHZnXu0_o8Qp"
      },
      "source": [
        "## tune this please\n",
        "params = {'replay_size':      100000, # not recommend\n",
        "        'replay_initial':   10000, # not recommend\n",
        "        'target_net_sync':  1000, # at least larger\n",
        "        'epsilon_frames':   10**5, # not recommend\n",
        "        'epsilon_start':    1.0, # not recommend\n",
        "        'epsilon_final':    0.02, # can be slightly larger/smaller\n",
        "        'learning_rate':    0.0001, # smaller the better\n",
        "        'gamma':   0.99, #\n",
        "        'batch_size':32,\n",
        "        \"stop_reward\": 15} # the game will end if you get stop_reard\n",
        "        "
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pU-TkIH4DuEE"
      },
      "source": [
        ""
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMLSfjWwnRiL"
      },
      "source": [
        "net = DuelingDQNModel(env.observation_space.shape,env.action_space.n).to(device)\n",
        "# # reload\n",
        "# try:\n",
        "#   net.load_state_dict(torch.load(GYM_NAME+\"-best.dat\"))\n",
        "# except:\n",
        "#   print(\"You do not have any model trained before\")"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSYCkjCNnYZK"
      },
      "source": [
        "\n",
        "# 1. loss function agent-target_network \n",
        "# 2. target_network sync with agent weighings \n",
        "\n",
        "# target_network \n",
        "target_network = ptan.agent.TargetNet(net)\n",
        "# epsilon greedy selector\n",
        "selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=params['epsilon_start'])\n",
        "# agent_network\n",
        "agent = ptan.agent.DQNAgent(net, selector, device=device)\n"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNO1KCLiok9s"
      },
      "source": [
        "# check library, it will output the steps-sequence , those sequence is generated by our env and our agent\n",
        "exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=params['gamma'], steps_count=n)\n",
        "buffer = ptan.experience.ExperienceReplayBuffer(exp_source, buffer_size=params['replay_size'])\n",
        "optimizer = optim.Adam(net.parameters(), lr=params['learning_rate'])\n"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pc5IQXIvkm2N"
      },
      "source": [
        ""
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15H3sHwZk4TE"
      },
      "source": [
        ""
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfToq79Ep4d4"
      },
      "source": [
        "## Loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRf6zr1o0v2h"
      },
      "source": [
        "def unpack_batch(batch):\n",
        "  # sort the batch and return states, actions, rewards, dones, last_states \n",
        "  states, actions, rewards, is_dones, last_states = [], [], [], [], []\n",
        "  for experience in batch:\n",
        "    state = np.array(experience.state,copy=False) # deep copy is needed\n",
        "    # sort the array here\n",
        "    states.append(state)\n",
        "    actions.append(experience.action)\n",
        "    rewards.append(experience.reward)\n",
        "    is_dones.append(experience.last_state is None)\n",
        "\n",
        "    if experience.last_state is None:\n",
        "      last_states.append(state) # append dummy state, as we will set them to 0\n",
        "    else:\n",
        "      last_states.append(np.array(experience.last_state, copy=False)) # append experience last_states sources\n",
        "  return np.array(states, copy=False), np.array(actions), np.array(rewards, dtype=np.float32), \\\n",
        "           np.array(is_dones, dtype=np.uint8), np.array(last_states, copy=False)"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLT8DhRC0u8M"
      },
      "source": [
        ""
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXLh-ArZp6Gu"
      },
      "source": [
        "def get_loss(batch,net,target_network,GAMMA=params['gamma'],device=\"cpu\"):\n",
        "  states,actions,rewards,is_dones,next_states = unpack_batch(batch)\n",
        "  states_v = torch.tensor(states).to(device)\n",
        "  actions_v = torch.tensor(actions).to(device)\n",
        "  rewards_v = torch.tensor(rewards).to(device)\n",
        "  is_dones_v = torch.ByteTensor(is_dones).to(device)\n",
        "  next_states_v = torch.tensor(next_states).to(device)\n",
        "\n",
        " \n",
        "  \n",
        "  # current network \n",
        "\n",
        "  batch_action_table_output = net(states_v) # 2D array [batch_size,action_size] batch size * action size\n",
        "  batch_best_action = batch_action_table_output.gather(1,actions_v.unsqueeze(-1)) # if action take is [1,2,0] for batch [1,2,3] ,then we get the value of\n",
        "                                  # batch_action_table_output[1][1],batch_action_table_output[2][2],batch_action_table_output[3][0]\n",
        "                                  # check out the function of unsqueeze yourself\n",
        "  batch_best_action_expanded = batch_best_action.squeeze(-1) # we need to add 1 more dimension on the array \n",
        "  current_state_action_value = batch_best_action_expanded\n",
        "  \n",
        "\n",
        "  # target network\n",
        "  # same theory as above but maximize is enough\n",
        "  next_state_batch_action_table_output = target_network(next_states_v)\n",
        "  next_state_values = next_state_batch_action_table_output.max(1)[0] # [0] is used as next_state_batch_action_table_output = [[1,2,3,4]]\n",
        "\n",
        "  # next_state_values = 0 if it is end already, we will not get any future value after it is die\n",
        "  next_state_values[is_dones_v] = 0.0\n",
        "\n",
        "  # we do not train target_net work so we need to detach it from the graph \n",
        "  next_state_values = next_state_values.detach()\n",
        "\n",
        "  # next_best_score * Gamma + current_transitional_reward\n",
        "  expected_state_action_value = next_state_values * GAMMA + rewards_v\n",
        "\n",
        "  loss_value = nn.MSELoss()(current_state_action_value,expected_state_action_value)\n",
        "  return loss_value"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odpq45qL4HiS"
      },
      "source": [
        ""
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSQh179uFkQV"
      },
      "source": [
        "## Epsilon tracker"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUu8Xh_iFnDP"
      },
      "source": [
        "# it helps decay the epsilon\n",
        "class EpsilonTracker:\n",
        "  def __init__(self, epsilon_greedy_selector, params):\n",
        "      self.epsilon_greedy_selector = epsilon_greedy_selector\n",
        "      self.epsilon_start = params['epsilon_start']\n",
        "      self.epsilon_final = params['epsilon_final']\n",
        "      self.epsilon_frames = params['epsilon_frames']\n",
        "      self.frame(0)\n",
        "\n",
        "  def frame(self, frame):\n",
        "      self.epsilon_greedy_selector.epsilon = max(self.epsilon_final, self.epsilon_start - frame / self.epsilon_frames)"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-Z7_bbmF2jL"
      },
      "source": [
        "epsilon_tracker = EpsilonTracker(selector,params)"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4qKpd9WoIkI"
      },
      "source": [
        "## Reward Tracker"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDBml3xHoGy3"
      },
      "source": [
        "class RewardTracker:\n",
        "    def __init__(self, writer, stop_reward):\n",
        "        self.writer = writer\n",
        "        self.stop_reward = stop_reward\n",
        "\n",
        "    def __enter__(self):\n",
        "        self.ts = time.time()\n",
        "        self.ts_frame = 0\n",
        "        self.total_rewards = []\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, *args):\n",
        "        self.writer.close()\n",
        "\n",
        "    def reward(self, reward, frame, epsilon=None):\n",
        "        self.total_rewards.append(reward)\n",
        "        speed = (frame - self.ts_frame) / (time.time() - self.ts)\n",
        "        self.ts_frame = frame\n",
        "        self.ts = time.time()\n",
        "        mean_reward = np.mean(self.total_rewards[-100:])\n",
        "        epsilon_str = \"\" if epsilon is None else \", eps %.2f\" % epsilon\n",
        "        print(\"%d: done %d games, mean reward %.3f, speed %.2f f/s%s\" % (\n",
        "            frame, len(self.total_rewards), mean_reward, speed, epsilon_str\n",
        "        ))\n",
        "        sys.stdout.flush()\n",
        "        if epsilon is not None:\n",
        "            self.writer.add_scalar(\"epsilon\", epsilon, frame)\n",
        "        self.writer.add_scalar(\"speed\", speed, frame)\n",
        "        self.writer.add_scalar(\"reward_100\", mean_reward, frame)\n",
        "        self.writer.add_scalar(\"reward\", reward, frame)\n",
        "        if mean_reward > self.stop_reward:\n",
        "            print(\"Solved in %d frames!\" % frame)\n",
        "            return True\n",
        "        return False\n",
        "# define writer\n",
        "writer = SummaryWriter(comment=\"-\" + \"Pong-dueling\")"
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N27WtFugpPCl"
      },
      "source": [
        "## Training epoch\n",
        "### Capture the final mean reward (mean 100)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H748cZE1pPIc",
        "outputId": "2de91e4c-1f4f-4de5-93bf-32ad477a80b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "frame_idx = 0\n",
        "\n",
        "with RewardTracker(writer, params['stop_reward']) as reward_tracker:\n",
        "    while True:\n",
        "        frame_idx += 1\n",
        "        buffer.populate(1)\n",
        "        epsilon_tracker.frame(frame_idx)\n",
        "\n",
        "        new_rewards = exp_source.pop_total_rewards()\n",
        "        if new_rewards:\n",
        "          if reward_tracker.reward(new_rewards[0], frame_idx, selector.epsilon):\n",
        "            break\n",
        "\n",
        "        if len(buffer) < params['replay_initial']:\n",
        "          continue\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        batch = buffer.sample(params['batch_size'])\n",
        "        loss_v = get_loss(batch, net, target_network.target_model, GAMMA=params['gamma'], device=device)\n",
        "        loss_v.backward()\n",
        "        optimizer.step()\n",
        "      \n",
        "\n",
        "        if frame_idx % params['target_net_sync'] == 0:\n",
        "          target_network.sync()"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "954: done 1 games, mean reward -20.000, speed 80.28 f/s, eps 0.99\n",
            "1967: done 2 games, mean reward -19.500, speed 80.34 f/s, eps 0.98\n",
            "2901: done 3 games, mean reward -20.000, speed 80.88 f/s, eps 0.97\n",
            "3994: done 4 games, mean reward -19.750, speed 81.10 f/s, eps 0.96\n",
            "4806: done 5 games, mean reward -20.000, speed 81.50 f/s, eps 0.95\n",
            "5779: done 6 games, mean reward -19.833, speed 81.22 f/s, eps 0.94\n",
            "6737: done 7 games, mean reward -20.000, speed 81.13 f/s, eps 0.93\n",
            "7635: done 8 games, mean reward -20.125, speed 81.06 f/s, eps 0.92\n",
            "8454: done 9 games, mean reward -20.222, speed 80.97 f/s, eps 0.92\n",
            "9677: done 10 games, mean reward -19.900, speed 81.01 f/s, eps 0.90\n",
            "10637: done 11 games, mean reward -20.000, speed 81.10 f/s, eps 0.89\n",
            "11725: done 12 games, mean reward -20.000, speed 80.69 f/s, eps 0.88\n",
            "12791: done 13 games, mean reward -20.000, speed 81.37 f/s, eps 0.87\n",
            "13843: done 14 games, mean reward -20.071, speed 79.86 f/s, eps 0.86\n",
            "15021: done 15 games, mean reward -19.933, speed 80.88 f/s, eps 0.85\n",
            "16022: done 16 games, mean reward -20.000, speed 80.97 f/s, eps 0.84\n",
            "16841: done 17 games, mean reward -20.059, speed 80.41 f/s, eps 0.83\n",
            "18042: done 18 games, mean reward -19.944, speed 80.89 f/s, eps 0.82\n",
            "19226: done 19 games, mean reward -19.947, speed 80.98 f/s, eps 0.81\n",
            "20640: done 20 games, mean reward -19.850, speed 80.75 f/s, eps 0.79\n",
            "21945: done 21 games, mean reward -19.810, speed 81.00 f/s, eps 0.78\n",
            "23061: done 22 games, mean reward -19.864, speed 81.40 f/s, eps 0.77\n",
            "24385: done 23 games, mean reward -19.870, speed 80.81 f/s, eps 0.76\n",
            "25444: done 24 games, mean reward -19.875, speed 81.21 f/s, eps 0.75\n",
            "26815: done 25 games, mean reward -19.720, speed 81.00 f/s, eps 0.73\n",
            "27941: done 26 games, mean reward -19.731, speed 80.89 f/s, eps 0.72\n",
            "29628: done 27 games, mean reward -19.630, speed 80.91 f/s, eps 0.70\n",
            "30948: done 28 games, mean reward -19.643, speed 80.68 f/s, eps 0.69\n",
            "32190: done 29 games, mean reward -19.655, speed 81.24 f/s, eps 0.68\n",
            "33368: done 30 games, mean reward -19.700, speed 81.41 f/s, eps 0.67\n",
            "34790: done 31 games, mean reward -19.677, speed 81.48 f/s, eps 0.65\n",
            "36626: done 32 games, mean reward -19.656, speed 81.20 f/s, eps 0.63\n",
            "38149: done 33 games, mean reward -19.606, speed 80.65 f/s, eps 0.62\n",
            "39703: done 34 games, mean reward -19.559, speed 80.46 f/s, eps 0.60\n",
            "41330: done 35 games, mean reward -19.543, speed 80.91 f/s, eps 0.59\n",
            "42562: done 36 games, mean reward -19.528, speed 81.35 f/s, eps 0.57\n",
            "44118: done 37 games, mean reward -19.486, speed 80.53 f/s, eps 0.56\n",
            "45860: done 38 games, mean reward -19.474, speed 81.10 f/s, eps 0.54\n",
            "47732: done 39 games, mean reward -19.359, speed 81.40 f/s, eps 0.52\n",
            "49131: done 40 games, mean reward -19.375, speed 81.27 f/s, eps 0.51\n",
            "51111: done 41 games, mean reward -19.317, speed 80.75 f/s, eps 0.49\n",
            "53186: done 42 games, mean reward -19.190, speed 81.05 f/s, eps 0.47\n",
            "55036: done 43 games, mean reward -19.093, speed 81.23 f/s, eps 0.45\n",
            "57072: done 44 games, mean reward -19.023, speed 81.16 f/s, eps 0.43\n",
            "58846: done 45 games, mean reward -18.956, speed 80.95 f/s, eps 0.41\n",
            "60787: done 46 games, mean reward -18.870, speed 80.85 f/s, eps 0.39\n",
            "62912: done 47 games, mean reward -18.830, speed 81.07 f/s, eps 0.37\n",
            "65392: done 48 games, mean reward -18.667, speed 80.39 f/s, eps 0.35\n",
            "67437: done 49 games, mean reward -18.653, speed 80.88 f/s, eps 0.33\n",
            "69272: done 50 games, mean reward -18.600, speed 81.20 f/s, eps 0.31\n",
            "72371: done 51 games, mean reward -18.333, speed 81.07 f/s, eps 0.28\n",
            "75019: done 52 games, mean reward -18.173, speed 81.54 f/s, eps 0.25\n",
            "77572: done 53 games, mean reward -18.057, speed 81.36 f/s, eps 0.22\n",
            "80297: done 54 games, mean reward -17.981, speed 81.36 f/s, eps 0.20\n",
            "82837: done 55 games, mean reward -17.945, speed 81.34 f/s, eps 0.17\n",
            "85343: done 56 games, mean reward -17.857, speed 81.14 f/s, eps 0.15\n",
            "89047: done 57 games, mean reward -17.649, speed 81.01 f/s, eps 0.11\n",
            "93143: done 58 games, mean reward -17.414, speed 81.25 f/s, eps 0.07\n",
            "96806: done 59 games, mean reward -17.220, speed 81.44 f/s, eps 0.03\n",
            "99678: done 60 games, mean reward -17.100, speed 82.25 f/s, eps 0.02\n",
            "103662: done 61 games, mean reward -16.754, speed 82.40 f/s, eps 0.02\n",
            "107661: done 62 games, mean reward -16.468, speed 82.36 f/s, eps 0.02\n",
            "110406: done 63 games, mean reward -16.349, speed 82.25 f/s, eps 0.02\n",
            "114088: done 64 games, mean reward -16.109, speed 82.11 f/s, eps 0.02\n",
            "117851: done 65 games, mean reward -15.892, speed 82.55 f/s, eps 0.02\n",
            "121630: done 66 games, mean reward -15.697, speed 82.43 f/s, eps 0.02\n",
            "125041: done 67 games, mean reward -15.612, speed 82.64 f/s, eps 0.02\n",
            "128461: done 68 games, mean reward -15.324, speed 82.62 f/s, eps 0.02\n",
            "132094: done 69 games, mean reward -15.174, speed 82.47 f/s, eps 0.02\n",
            "135689: done 70 games, mean reward -14.971, speed 82.70 f/s, eps 0.02\n",
            "139318: done 71 games, mean reward -14.803, speed 82.38 f/s, eps 0.02\n",
            "142215: done 72 games, mean reward -14.472, speed 82.30 f/s, eps 0.02\n",
            "145170: done 73 games, mean reward -14.137, speed 82.81 f/s, eps 0.02\n",
            "149027: done 74 games, mean reward -13.905, speed 82.75 f/s, eps 0.02\n",
            "152425: done 75 games, mean reward -13.587, speed 82.68 f/s, eps 0.02\n",
            "154462: done 76 games, mean reward -13.145, speed 82.58 f/s, eps 0.02\n",
            "156656: done 77 games, mean reward -12.727, speed 82.58 f/s, eps 0.02\n",
            "159390: done 78 games, mean reward -12.372, speed 82.76 f/s, eps 0.02\n",
            "162512: done 79 games, mean reward -12.114, speed 82.71 f/s, eps 0.02\n",
            "164835: done 80 games, mean reward -11.750, speed 82.49 f/s, eps 0.02\n",
            "167900: done 81 games, mean reward -11.469, speed 82.25 f/s, eps 0.02\n",
            "170190: done 82 games, mean reward -11.134, speed 82.39 f/s, eps 0.02\n",
            "172530: done 83 games, mean reward -10.783, speed 82.89 f/s, eps 0.02\n",
            "175552: done 84 games, mean reward -10.476, speed 82.89 f/s, eps 0.02\n",
            "178020: done 85 games, mean reward -10.153, speed 82.61 f/s, eps 0.02\n",
            "180159: done 86 games, mean reward -9.814, speed 82.63 f/s, eps 0.02\n",
            "182952: done 87 games, mean reward -9.540, speed 82.37 f/s, eps 0.02\n",
            "185763: done 88 games, mean reward -9.261, speed 82.91 f/s, eps 0.02\n",
            "187851: done 89 games, mean reward -8.933, speed 83.52 f/s, eps 0.02\n",
            "190016: done 90 games, mean reward -8.622, speed 82.86 f/s, eps 0.02\n",
            "192318: done 91 games, mean reward -8.374, speed 82.89 f/s, eps 0.02\n",
            "194703: done 92 games, mean reward -8.098, speed 83.69 f/s, eps 0.02\n",
            "197189: done 93 games, mean reward -7.828, speed 83.32 f/s, eps 0.02\n",
            "199262: done 94 games, mean reward -7.543, speed 83.59 f/s, eps 0.02\n",
            "201466: done 95 games, mean reward -7.274, speed 83.58 f/s, eps 0.02\n",
            "203909: done 96 games, mean reward -7.021, speed 83.02 f/s, eps 0.02\n",
            "206323: done 97 games, mean reward -6.794, speed 83.16 f/s, eps 0.02\n",
            "208904: done 98 games, mean reward -6.561, speed 83.39 f/s, eps 0.02\n",
            "211281: done 99 games, mean reward -6.323, speed 83.17 f/s, eps 0.02\n",
            "213625: done 100 games, mean reward -6.100, speed 83.48 f/s, eps 0.02\n",
            "215716: done 101 games, mean reward -5.700, speed 82.97 f/s, eps 0.02\n",
            "217885: done 102 games, mean reward -5.320, speed 82.79 f/s, eps 0.02\n",
            "220117: done 103 games, mean reward -4.950, speed 83.38 f/s, eps 0.02\n",
            "222363: done 104 games, mean reward -4.570, speed 83.39 f/s, eps 0.02\n",
            "224439: done 105 games, mean reward -4.160, speed 83.20 f/s, eps 0.02\n",
            "226464: done 106 games, mean reward -3.770, speed 83.37 f/s, eps 0.02\n",
            "228461: done 107 games, mean reward -3.350, speed 83.19 f/s, eps 0.02\n",
            "230815: done 108 games, mean reward -2.970, speed 82.27 f/s, eps 0.02\n",
            "233250: done 109 games, mean reward -2.620, speed 82.63 f/s, eps 0.02\n",
            "235421: done 110 games, mean reward -2.270, speed 83.20 f/s, eps 0.02\n",
            "237681: done 111 games, mean reward -1.870, speed 83.24 f/s, eps 0.02\n",
            "240192: done 112 games, mean reward -1.500, speed 83.94 f/s, eps 0.02\n",
            "242194: done 113 games, mean reward -1.100, speed 82.72 f/s, eps 0.02\n",
            "244931: done 114 games, mean reward -0.760, speed 84.05 f/s, eps 0.02\n",
            "247446: done 115 games, mean reward -0.410, speed 84.92 f/s, eps 0.02\n",
            "249938: done 116 games, mean reward -0.050, speed 84.37 f/s, eps 0.02\n",
            "252144: done 117 games, mean reward 0.340, speed 84.55 f/s, eps 0.02\n",
            "254494: done 118 games, mean reward 0.690, speed 84.54 f/s, eps 0.02\n",
            "256834: done 119 games, mean reward 1.060, speed 84.56 f/s, eps 0.02\n",
            "259061: done 120 games, mean reward 1.430, speed 84.42 f/s, eps 0.02\n",
            "261152: done 121 games, mean reward 1.800, speed 84.43 f/s, eps 0.02\n",
            "263083: done 122 games, mean reward 2.210, speed 84.31 f/s, eps 0.02\n",
            "265672: done 123 games, mean reward 2.530, speed 84.63 f/s, eps 0.02\n",
            "267864: done 124 games, mean reward 2.910, speed 84.41 f/s, eps 0.02\n",
            "269842: done 125 games, mean reward 3.260, speed 83.44 f/s, eps 0.02\n",
            "272055: done 126 games, mean reward 3.650, speed 83.89 f/s, eps 0.02\n",
            "274012: done 127 games, mean reward 3.990, speed 84.52 f/s, eps 0.02\n",
            "276271: done 128 games, mean reward 4.360, speed 84.67 f/s, eps 0.02\n",
            "278438: done 129 games, mean reward 4.740, speed 84.44 f/s, eps 0.02\n",
            "280891: done 130 games, mean reward 5.060, speed 84.73 f/s, eps 0.02\n",
            "283592: done 131 games, mean reward 5.380, speed 84.69 f/s, eps 0.02\n",
            "285275: done 132 games, mean reward 5.780, speed 84.60 f/s, eps 0.02\n",
            "287253: done 133 games, mean reward 6.150, speed 84.85 f/s, eps 0.02\n",
            "289096: done 134 games, mean reward 6.540, speed 84.94 f/s, eps 0.02\n",
            "291143: done 135 games, mean reward 6.920, speed 84.80 f/s, eps 0.02\n",
            "293004: done 136 games, mean reward 7.310, speed 84.87 f/s, eps 0.02\n",
            "295245: done 137 games, mean reward 7.670, speed 85.23 f/s, eps 0.02\n",
            "297073: done 138 games, mean reward 8.070, speed 84.68 f/s, eps 0.02\n",
            "298928: done 139 games, mean reward 8.420, speed 85.21 f/s, eps 0.02\n",
            "300713: done 140 games, mean reward 8.820, speed 85.18 f/s, eps 0.02\n",
            "302727: done 141 games, mean reward 9.180, speed 85.00 f/s, eps 0.02\n",
            "304586: done 142 games, mean reward 9.510, speed 84.76 f/s, eps 0.02\n",
            "306868: done 143 games, mean reward 9.800, speed 85.03 f/s, eps 0.02\n",
            "308778: done 144 games, mean reward 10.140, speed 84.97 f/s, eps 0.02\n",
            "311193: done 145 games, mean reward 10.460, speed 84.53 f/s, eps 0.02\n",
            "313188: done 146 games, mean reward 10.800, speed 84.46 f/s, eps 0.02\n",
            "315394: done 147 games, mean reward 11.150, speed 84.77 f/s, eps 0.02\n",
            "317239: done 148 games, mean reward 11.460, speed 84.50 f/s, eps 0.02\n",
            "318935: done 149 games, mean reward 11.840, speed 84.21 f/s, eps 0.02\n",
            "321139: done 150 games, mean reward 12.180, speed 84.98 f/s, eps 0.02\n",
            "322969: done 151 games, mean reward 12.440, speed 84.03 f/s, eps 0.02\n",
            "324900: done 152 games, mean reward 12.720, speed 84.84 f/s, eps 0.02\n",
            "326876: done 153 games, mean reward 13.030, speed 84.93 f/s, eps 0.02\n",
            "328738: done 154 games, mean reward 13.360, speed 84.71 f/s, eps 0.02\n",
            "330501: done 155 games, mean reward 13.710, speed 84.95 f/s, eps 0.02\n",
            "332203: done 156 games, mean reward 14.040, speed 84.41 f/s, eps 0.02\n",
            "334214: done 157 games, mean reward 14.280, speed 84.61 f/s, eps 0.02\n",
            "336072: done 158 games, mean reward 14.500, speed 84.29 f/s, eps 0.02\n",
            "337985: done 159 games, mean reward 14.750, speed 83.81 f/s, eps 0.02\n",
            "339723: done 160 games, mean reward 15.050, speed 83.92 f/s, eps 0.02\n",
            "Solved in 339723 frames!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUlmJf7BsDss"
      },
      "source": [
        "print(net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Wv2vGHd7-R_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}